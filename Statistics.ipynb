{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfc38390",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d90ad269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import poisson as pois\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nbimporter\n",
    "import prepData as prep\n",
    "import fitFunc as fits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f5d2aa",
   "metadata": {},
   "source": [
    "## Significance Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a852cb6a",
   "metadata": {},
   "source": [
    "We want to compute the significance of the observed dataset.  \n",
    "The first steps are rescaling the dataset and fitting the background and signal functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6e851d",
   "metadata": {},
   "source": [
    "We then compute the likelihood ratio of the observed data $q_0^{obs}$ between the signal and null hypothesis:\n",
    "\n",
    "$$q_0^{obs} = -2 \\cdot \\log \\left( \\cfrac{\\mathcal{L} \\left(Data | 0, \\hat{\\theta}_0 \\right)}{\\mathcal{L} \\left(Data | \\hat{\\mu}, \\hat{\\theta}_\\hat{\\mu} \\right)}\\right)$$\n",
    "\n",
    "The ^ symbol indicates the values that optimize the fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e88568a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute likelihood ratios of two hypotheses\n",
    "def lh_ratio(y, model_null, model_alt):\n",
    "    # compute log likelihoods\n",
    "    LogLike_null = sum(norm.logpdf(x=y, loc=model_null.best_fit, scale=np.sqrt(y)))\n",
    "    LogLike_alt  = sum(norm.logpdf(x=y, loc=model_alt.best_fit,  scale=np.sqrt(y)))\n",
    "    \n",
    "    #LogLike_null = sum(pois.logpmf(fft.astype(int), model_null.best_fit.astype(int)))\n",
    "    #LogLike_alt  = sum(pois.logpmf(fft.astype(int), model_alt.best_fit.astype(int)))\n",
    "    \n",
    "    # ratio\n",
    "    q = -2 * (LogLike_null - LogLike_alt)\n",
    "    \n",
    "    return(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f59fa",
   "metadata": {},
   "source": [
    "The value of $q_0^{obs}$ has no meaning by itself, so we generate n = 10,000 toy datasets from the expected values given by the background fit and repeate the analysis for every new dataset. Both for the likelihood ratio and for the toy dataset generation, a normal approximation has been used instead of the formal Poisson distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed872673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_toydataset(values, n):\n",
    "    toy_dataset = norm.rvs(loc=values, scale=np.sqrt(values), size=(n,len(values)))\n",
    "    \n",
    "    #toy_dataset = pois.rvs(mu=values, size=(n,len(values)))\n",
    "    \n",
    "    return(toy_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9f79d2",
   "metadata": {},
   "source": [
    "The original $q_0^{obs}$ is thus compared with the distribution of $q_0$ obtained from the toy datasets, and the p-value is computed:\n",
    "\n",
    "$$p_0 = P \\left( q_0 \\ge q_0^{obs} \\right) = \\int_{q_0^{obs}}^{+\\infty} f(q_0 | 0, \\hat{\\theta}_0) \\,dx $$ \n",
    "\n",
    "The significance is expressed as the number of $\\sigma$s needed to achieve an equivalent p-value in a standard normal deviation:\n",
    "\n",
    "$$z = \\Phi^{-1} \\left(1 - p_0 \\right)$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09111118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_value(q, q_obs):\n",
    "    p0 = sum(q >= q_obs)/len(q)\n",
    "    return(p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a74623",
   "metadata": {},
   "source": [
    "This process is repeted using every possible frequency as $x_0$, the center of the signal function.  \n",
    "The complete code is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cff84ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig_test(run, signal, path='db/', n=10000):\n",
    "    # load and prep data\n",
    "    data,center,length = prep.load_dataset(run, path)\n",
    "    freq, fft, weights, ref = prep.prep_data(data,center,length=length)\n",
    "    res_bkg = fits.fit_bkg(freq, fft, weights, center, ref)\n",
    "    \n",
    "    # scan x0\n",
    "    z = np.zeros(len(freq))\n",
    "    for i in range(len(freq)):\n",
    "        z[i] = significance(freq, fft, weights, center, freq.values[i], res_bkg, signal, n)\n",
    "    \n",
    "    return(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a348477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def significance(freq, fft, weights, res_bkg, center, ref, x_0, signal, n=10000, draw = False):\n",
    "    res_sig = fits.fit_sig(freq, fft, weights, x_0, res_bkg, signal)\n",
    "    \n",
    "    # compute likelihood ratio of observed data\n",
    "    q0_obs = lh_ratio(fft, res_bkg, res_sig)\n",
    "    \n",
    "    # generate toy datasets and compute likelihood ratio for all of them\n",
    "    toy_fft = gen_toydataset(res_bkg.best_fit, n)\n",
    "    toy_weights = toy_fft/np.sqrt(1365500) # NOT UP TO DATE !!!!!!!!!!!!!!!!\n",
    "    \n",
    "    q0 = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        toy_bkg = fits.fit_bkg(freq, toy_fft[i], toy_weights[i], center, ref)\n",
    "        toy_sig = fits.fit_sig(freq, toy_fft[i], toy_weights[i], x_0, toy_bkg, signal)\n",
    "        q0[i] = lh_ratio(toy_fft[i], toy_bkg, toy_sig)\n",
    "    \n",
    "    # plot significance distribution\n",
    "    if(draw):\n",
    "        plot_significance(q0, q0_obs)\n",
    "        \n",
    "    # compute significance\n",
    "    p0 = p_value(q0, q0_obs)\n",
    "    z = norm.ppf(1-p0)\n",
    "        \n",
    "    return(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a01f213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_significance(q0, q0_obs):\n",
    "    # prepare canvas\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    \n",
    "    # plot q0 distribution\n",
    "    N = len(q0)\n",
    "    binning = int(np.sqrt(N))\n",
    "    n, bins, _ = plt.hist(q0, bins=binning, density = True, \n",
    "                          facecolor='lightblue', edgecolor='black', label='Toy Experiments')\n",
    "    plt.vlines(q0_obs, 0, max(n), colors='blue', linestyles='--', label='Observed Data')\n",
    "    \n",
    "    plt.legend(loc='upper left')\n",
    "    plt.xlabel('q0')\n",
    "    plt.ylabel('PDf')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b8a1fd",
   "metadata": {},
   "source": [
    "## Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f5cf72",
   "metadata": {},
   "source": [
    "The process is similar to that of the significance test, but with a few key differences.  \n",
    "\n",
    "In addition to fitting the background, we fit the signal twice: one time we let the $\\mu$ run free to find $\\hat{\\mu}$, while the other we keep it fixed to a certain value.  \n",
    "The likelihood ratio is thus computed as:\n",
    "\n",
    "$$q^{obs}(\\mu) = -2 \\cdot \\log \\left( \\cfrac{\\mathcal{L} \\left(Data | \\mu, \\hat{\\theta}_{\\mu} \\right)}{\\mathcal{L} \\left(Data | \\hat{\\mu}, \\hat{\\theta}_\\hat{\\mu} \\right)}\\right)$$\n",
    "\n",
    "We then generate two sets of n = 10,000 toy datasets each, one as before from the expected values given by the background fit while the other from the signal fit with fixed $\\mu$. We compare the original $q^{obs}(\\mu)$ with the distribution of $q(0)$ and $q(\\mu)$ from the toy datasets and compute the two probabilities:\n",
    "\n",
    "$$    p_{\\mu} = P \\left(q(\\mu) \\ge q^{obs}(\\mu) | \\mu s + b \\right)$$\n",
    "$$1 - p_{b}   = P \\left(q(\\mu) \\ge q^{obs}(\\mu) | b \\right)$$\n",
    "\n",
    "and take their ratio.  \n",
    "This process is done scanning different values of $\\mu$ and we take as the 95% confidence interval limit the value of $\\mu$ so that the ratio is equal to 0.05:\n",
    "\n",
    "$$CL: \\mu \\; \\big| \\; \\cfrac{p_{\\mu}}{1 - p_{b}} = 0.05$$\n",
    "\n",
    "This process is repeted using every possible frequency as $x_0$, the center of the signal function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2b89b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CI_test(run, signal, path='db/', n=10000):\n",
    "    # load and prep data\n",
    "    data,center,length = prep.load_dataset(run, path)\n",
    "    freq, fft, weights, ref = prep.prep_data(data,center,length=length)\n",
    "    res_bkg = fits.fit_bkg(freq, fft, weights, center, ref)\n",
    "    \n",
    "    # scan x0\n",
    "    mu_ci = np.zeros(len(freq))\n",
    "    for i in range(len(freq)):\n",
    "        mu_ci[i] = CI(freq, fft, weights, center, freq.values[i], res_bkg, signal, n)\n",
    "    \n",
    "    return(mu_ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b636b10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CI(freq, fft, weights, res_bkg, center, ref, x_0, signal, n=10000, verbose = False):\n",
    "    # scan different mu\n",
    "    mu_vals = np.array([1e-4, 5e-5, 1e-5, 5e-6, 1e-6, 5e-7])*fft[np.argmin(fft)]\n",
    "    r = np.zeros(len(mu_vals))\n",
    "    \n",
    "    for i in range(len(r)):\n",
    "        res_fix = fits.fit_sig(freq, fft, weights, x_0, res_bkg, signal, mu_init = mu_vals[i], mu_vary = False)\n",
    "        res_sig = fits.fit_sig(freq, fft, weights, x_0, res_bkg, signal)\n",
    "        \n",
    "        # compute likelihood ratio of observed data\n",
    "        q_mu_obs = lh_ratio(fft, res_fix, res_sig)\n",
    "        \n",
    "        # generate toy datasets and compute likelihood ratio\n",
    "        \n",
    "        # fixed mu\n",
    "        toy_fft_mu     = gen_toydataset(res_fix.best_fit, n)\n",
    "        toy_weights_mu = toy_fft_mu/np.sqrt(1365500) # NOT UP TO DATE !!!!!!!!!!!!!!!!\n",
    "        q_mu   = np.zeros(n)\n",
    "        for j in range(n):\n",
    "            toy_bkg = fits.fit_bkg(freq, toy_fft_mu[j], toy_weights_mu[j], center, ref)\n",
    "            toy_fix = fits.fit_sig(freq, toy_fft_mu[j], toy_weights_mu[j], x_0, toy_bkg, signal,\n",
    "                                   mu_init = mu_vals[i], mu_vary = False)\n",
    "            toy_sig = fits.fit_sig(freq, toy_fft_mu[j], toy_weights_mu[j], x_0, toy_bkg, signal)\n",
    "            q_mu[i] = lh_ratio(toy_fft_mu[j], toy_fix, toy_sig)\n",
    "        \n",
    "        # background\n",
    "        toy_fft_bkg     = gen_toydataset(res_bkg.best_fit, n)\n",
    "        toy_weights_bkg = toy_fft_bkg/np.sqrt(1365500) # NOT UP TO DATE !!!!!!!!!!!!!!!!\n",
    "        q0   = np.zeros(n)\n",
    "        for j in range(n):\n",
    "            toy_bkg = fits.fit_bkg(freq, toy_fft_bkg[j], toy_weights_bkg[j], center, ref)\n",
    "            toy_sig = fits.fit_sig(freq, toy_fft_bkg[j], toy_weights_bkg[j], x_0, toy_bkg, signal)\n",
    "            q0[j] = lh_ratio(toy_fft_bkg[j], toy_bkg, toy_sig)\n",
    "        \n",
    "        # compute p-values\n",
    "        p_mu = p_value(q_mu, q_mu_obs)\n",
    "        p_b  = p_value(q0, q_mu_obs)\n",
    "        \n",
    "        # compute ratio\n",
    "        r[i] = p_mu/p_b\n",
    "        \n",
    "        if(verbose):\n",
    "            print(\"Mu: \", mu_vals[i], \"\\np_mu =\", p_mu, \"  p_b =\", p_b, \"  ratio =\", r[i], \"\\n\")\n",
    "    \n",
    "    idx = np.argmin(np.abs(r - 0.05))\n",
    "        \n",
    "    return(mu_vals[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
